{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三层神经网络选择器\n",
    "来源于论文PET-select中的选择器部分 https://arxiv.org/abs/2409.16416"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要是通过微调后的codeBERT生成的问题向量进行输入，通过三层全连接网络，最后通过一个softmax函数输出各个类别的概率，进而来判断应该使用哪种PET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import compute_class_weight\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设定随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取嵌入向量的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def get_embedding(questions, model_path):\n",
    "    \"\"\"\n",
    "    获取问题的嵌入表示。\n",
    "\n",
    "    Args:\n",
    "        questions (list of str): 需要获取嵌入表示的问题列表。\n",
    "        model_path (str): SentenceTransformer模型的路径。\n",
    "\n",
    "    Returns:\n",
    "        list of numpy.ndarray: 每个问题的嵌入表示列表，每个嵌入表示是一个numpy数组。\n",
    "\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_path)\n",
    "    embeddings = []\n",
    "    print(\"Generating embeddings...\")\n",
    "    for question in tqdm.tqdm(questions):\n",
    "        embedding = model.encode(question)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据和嵌入向量生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"code_complex\"\n",
    "hard = \"_hard\"\n",
    "data_type = \"_all\"\n",
    "file_path = f\"result/{dataset}_dataset/{dataset}_classification{hard}_dataset_train{data_type}.jsonl\"\n",
    "test_file_path = f\"result/{dataset}_dataset/{dataset}_classification{hard}_dataset_test{data_type}.jsonl\"\n",
    "model_path = f\"result/{dataset}_contrastive{hard}{data_type}_model\"\n",
    "data = pd.read_json(file_path, lines=True)\n",
    "test_data = pd.read_json(test_file_path, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = data[\"text\"].tolist()\n",
    "labels = data[\"label\"].tolist()\n",
    "ranks = data[\"rank\"].tolist()\n",
    "\n",
    "test_questions = test_data[\"text\"].tolist()\n",
    "test_labels = test_data[\"label\"].tolist()\n",
    "test_ranks = test_data[\"rank\"].tolist()\n",
    "\n",
    "embeddings = get_embedding(questions, model_path)\n",
    "test_embeddings = get_embedding(test_questions, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(embeddings[0])\n",
    "print(input_size)\n",
    "num_classes = 9\n",
    "print(num_classes)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分训练集、验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_embeddings,\n",
    "    eval_embeddings,\n",
    "    train_labels,\n",
    "    eval_labels,\n",
    "    train_ranks,\n",
    "    eval_ranks,\n",
    ") = train_test_split(embeddings, labels, ranks, test_size=0.2, random_state=42)\n",
    "test_embeddings, test_labels, test_ranks = test_embeddings, test_labels, test_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取自定义数据集格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels, ranks):\n",
    "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.ranks = ranks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx], self.ranks[idx]\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    DataLoader默认的数据合并方式可能不适用于所有情况，特别是当数据项具有复杂结构或需要特定处理时\n",
    "\n",
    "    对batch数据进行自定义处理，返回处理后的数据\n",
    "\n",
    "    Args:\n",
    "        batch (list): 包含多个tuple的list，每个tuple包含三个元素，分别是embeddings, labels, relevance_scores\n",
    "\n",
    "    Returns:\n",
    "        tuple: 包含处理后的embeddings, labels, relevance_scores。\n",
    "            embeddings (torch.Tensor): 将batch中所有样本的embeddings进行stack处理后的tensor\n",
    "            labels (torch.Tensor): 将batch中所有样本的labels转换为long类型的tensor\n",
    "            relevance_scores (list): batch中所有样本的relevance_scores的list\n",
    "    \"\"\"\n",
    "    embeddings = torch.stack([item[0] for item in batch])\n",
    "    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n",
    "    relevance_scores = [item[2] for item in batch]\n",
    "    return embeddings, labels, relevance_scores\n",
    "\n",
    "\n",
    "# 获得自定义数据集\n",
    "train_dataset = CustomDataset(train_embeddings, train_labels, train_ranks)\n",
    "eval_dataset = CustomDataset(eval_embeddings, eval_labels, eval_ranks)\n",
    "test_dataset = CustomDataset(test_embeddings, test_labels, test_ranks)\n",
    "\n",
    "# 获得自定义数据加载器\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "# 计算类别权重，处理类别不平衡问题，防止模型过拟合\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\", classes=np.unique(train_labels), y=train_labels\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三层全连接神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        # 调用父类的初始化函数\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        # 定义第一个全连接层，输入大小为input_size，输出大小为128\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        # 定义第二个全连接层，输入大小为128，输出大小为64\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # 定义第三个全连接层，输入大小为64，输出大小为num_classes（即分类的类别数）\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "        # 定义ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # 前向传播函数，定义了数据在模型中如何流动\n",
    "    def forward(self, x):\n",
    "        # 数据通过第一个全连接层后进行ReLU激活\n",
    "        x = self.relu(self.fc1(x))\n",
    "        # 数据通过第二个全连接层后进行ReLU激活\n",
    "        x = self.relu(self.fc2(x))\n",
    "        # 数据通过第三个全连接层，不进行激活\n",
    "        x = self.fc3(x)\n",
    "        # 对输出进行softmax操作，使得输出的每一行代表一个概率分布\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# create model, criterion, and optimizer 模型、损失函数和优化器（梯度下降的优化）\n",
    "model = ClassificationModel(input_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "# model.parameters(): 这个方法返回模型中所有可训练参数的迭代器。这些参数是优化器在每次迭代中需要更新的\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算 nDCG 和 MRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. `dcg`（Discounted Cumulative Gain）\n",
    "##### **功能**：\n",
    "\n",
    "- **DCG** 是一种评估排序结果质量的指标。它通过考虑相关性得分以及排名位置的影响，衡量结果集的累积增益。\n",
    "  \n",
    "##### **工作原理**：\n",
    "1. **输入**：\n",
    "   - `relevances`：一个列表，表示不同项目的相关性得分。\n",
    "   - `k`：我们只评估前`k`个项目。\n",
    "\n",
    "2. **处理**：\n",
    "   - 将相关性得分转换为浮点型数组，并截取前`k`个元素。\n",
    "   - 使用折扣公式： \n",
    "     $$\n",
    "     DCG = \\sum_{i=1}^{k} \\frac{\\text{relevance}_i}{\\log_2(i + 1)}\n",
    "     $$\n",
    "   - 每个相关性得分除以其位置的对数，位置越靠后，贡献越小（对数是从2开始以避免除以0）。\n",
    "   \n",
    "3. **返回**：  \n",
    "   - 如果相关性数组不为空，返回折扣后的累加值；否则返回`0.0`。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `ndcg`（Normalized Discounted Cumulative Gain）\n",
    "##### **功能**：\n",
    "- **nDCG** 是DCG的归一化版本，比较当前排序结果与理想排序结果的差距。\n",
    "\n",
    "##### **工作原理**：\n",
    "1. **输入**：\n",
    "   - `relevances`：原始相关性得分列表。\n",
    "   - `k`：评估前`k`个项目。\n",
    "\n",
    "2. **步骤**：\n",
    "   - 计算当前排序的DCG（调用`dcg`函数）。\n",
    "   - 计算理想排序（相关性从大到小排列）的DCG，即`IDCG`。\n",
    "   - 归一化：如果`IDCG`为0，则返回0，否则返回`DCG / IDCG`。\n",
    "\n",
    "3. **意义**：  \n",
    "   - 归一化后的nDCG范围在0到1之间，值越接近1，表示当前排序与理想排序越接近。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `calculate_mrr`（Mean Reciprocal Rank）\n",
    "\n",
    "##### **功能**：\n",
    "- **MRR**（平均倒数排名）是用来评估排序模型性能的指标之一，衡量预测中正确答案在所有可能答案中的排名，并取倒数排名的平均值。\n",
    "\n",
    "##### **工作原理**：\n",
    "1. **输入**：\n",
    "   - `outputs`：这是模型的预测得分，形状为 `[batch_size, num_classes]`，表示模型对每个样本的多个类别得分。\n",
    "   - `labels`：真实标签，形状为 `[batch_size]`，表示每个样本的正确类别。\n",
    "\n",
    "2. **处理**：\n",
    "   - **遍历每个样本**：\n",
    "     - 对每个样本，获取其所有类别的预测分数 `scores`。\n",
    "     - 提取当前样本的真实类别 `target`。\n",
    "     - 使用 `torch.sort` 将分数从高到低排序，得到排序后的索引 `sorted_indices`。\n",
    "     - 通过检查 `sorted_indices` 找到真实类别在排序中的排名 `rank`，并计算其倒数排名 `1/rank`。\n",
    "   - **累加倒数排名**：\n",
    "     - 将每个样本的倒数排名累加至 `mrr`。\n",
    "  \n",
    "3. **返回**：\n",
    "   - 取累积的倒数排名的平均值，作为 MRR 的输出。即：将所有样本的倒数排名加总后除以样本数。\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. `calculate_ndcg`（Normalized Discounted Cumulative Gain）\n",
    "\n",
    "##### **功能**：\n",
    "- **NDCG**（归一化折损累积增益）是一个用于评估模型排序性能的指标，比较了模型排序与理想排序的差异，并归一化到 [0, 1] 区间，1 表示排序完全正确。\n",
    "  \n",
    "##### **工作原理**：\n",
    "1. **输入**：\n",
    "   - `outputs`：模型输出的预测得分，形状为 `[batch_size, num_items]`，表示每个样本中不同物品的评分。\n",
    "   - `ranks`：真实标签，字典形式，每个 `item` 的ID对应一个标签（通常为 0 或 1，表示是否相关）。\n",
    "   - `k`：一个可选参数，表示只考虑前 `k` 个排序最高的项目，默认值为 6。\n",
    "\n",
    "2. **处理**：\n",
    "   - **遍历每个样本**：\n",
    "     - 对于每个样本，获取其真实的标签 `rank` 字典，表示各个 `item` 是否相关。\n",
    "     - 获取当前样本的模型输出得分 `scores`。\n",
    "     - 使用 `torch.sort` 对得分进行降序排序，得到排序后的分数和对应的 `item` 索引 `sorted_indices`。\n",
    "     - 根据排序后的索引，使用真实标签 `rank` 字典生成一个相关性列表 `relevances`，其中每个元素表示排序后的 `item` 在真实标签中对应的相关性。\n",
    "   - **计算 DCG**：\n",
    "     - 调用 `ndcg` 函数，基于 `relevances` 和前 `k` 个项目，计算该样本的 NDCG 值。\n",
    "     - 将每个样本的 NDCG 值累加到总的 NDCG 值 `ndcg_value` 中。\n",
    "  \n",
    "3. **返回**：\n",
    "   - 计算并返回所有样本的平均 NDCG 值，即将累积的 NDCG 值除以批次大小 `outputs.size(0)`。\n",
    "\n",
    "##### **意义**：\n",
    "- NDCG 的值越高，说明模型的排序越接近真实相关性排序，反映了模型在信息检索或排序任务中的性能。\n",
    "- 通过只考虑前 `k` 个项目，函数可以专注于模型最重要的排名结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances, k):\n",
    "    relevances = np.asarray(relevances, dtype=float)[:k]\n",
    "    if relevances.size:\n",
    "        return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def ndcg(relevances, k):\n",
    "    dcg_value = dcg(relevances, k)\n",
    "    idcg_value = dcg(sorted(relevances, reverse=True), k)\n",
    "    if idcg_value == 0:\n",
    "        return 0.0\n",
    "    return dcg_value / idcg_value\n",
    "\n",
    "\n",
    "# calcuate MRR\n",
    "def calculate_mrr(outputs, labels):\n",
    "    mrr = 0.0\n",
    "    for i in range(outputs.size(0)):\n",
    "        scores = outputs[i]\n",
    "        target = labels[i].item()\n",
    "        sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
    "        rank = (sorted_indices == target).nonzero(as_tuple=True)[0].item() + 1\n",
    "        mrr += 1.0 / rank\n",
    "    return mrr / outputs.size(0)\n",
    "\n",
    "\n",
    "# calculate ndcg\n",
    "def calculate_ndcg(outputs, ranks, k=6):\n",
    "    ndcg_value = 0.0\n",
    "    for i in range(outputs.size(0)):\n",
    "        rank = ranks[i]\n",
    "        scores = outputs[i]\n",
    "        sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
    "        relevances = np.array(\n",
    "            [rank[str(idx.item())] for idx in sorted_indices]\n",
    "        )\n",
    "        ndcg_value += ndcg(relevances, k) \n",
    "    return ndcg_value / outputs.size(0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 详细解释\n",
    "对于每个提示词的标记是这样的\n",
    "``` json\n",
    "{\n",
    "    \"Zeroshot\": 0,\n",
    "    \"Zeroshot_CoT\": 1,\n",
    "    \"Fewshot\": 2,\n",
    "    \"Fewshot_CoT\": 3,\n",
    "    \"SelfDebugl\": 4,\n",
    "    \"Reflection\": 5,\n",
    "    \"SelfPlan\": 6,\n",
    "    \"ProgressiveHint\": 7,\n",
    "    \"Persona\": 8\n",
    "}\n",
    "```\n",
    "然后、对于数据集中的每个样本，是长这样的。`label` 代表最适合的PET，`rank` 代表每个PET的得分，越合适得分越高，如果为 0 代表测试不通过\n",
    "``` json\n",
    "{\n",
    "    \"question\": \"……\",\n",
    "    \"label\": 0,\n",
    "    \"rank\": {\n",
    "        \"0\": 9,\n",
    "        \"2\": 8,\n",
    "        \"3\": 7,\n",
    "        \"4\": 6,\n",
    "        \"6\": 5,\n",
    "        \"7\": 4,\n",
    "        \"8\": 3,\n",
    "        \"1\": 0,\n",
    "        \"5\": 0\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟模型的输出\n",
    "\n",
    "假设模型的输出分数与真实排序极不一致，完全颠倒：\n",
    "``` python\n",
    "outputs = torch.tensor([[0.1, 0.9, 0.15, 0.2, 0.25, 0.95, 0.3, 0.35, 0.4]])\n",
    "```\n",
    "这表示：\n",
    "- 模型认为 `item` 5 和 `item` 1 的分数最高，分别为 0.95 和 0.9，而这两个 `item` 的真实相关性为 0。\n",
    "- 同时，模型认为真实相关性最高的 `item` 0 的得分最低，为 0.1。\n",
    "####  calculate_mrr\n",
    "**对得分排序**：\n",
    "- `sorted_scores, sorted_indices = torch.sort(scores, descending=True)`：对模型预测的所有类别得分进行降序排序，并获取相应的类别索引（排序后的类别）。\n",
    "- `sorted_scores = tensor([0.95, 0.9, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1])`\n",
    "- `sorted_indices = tensor([5, 1, 8, 7, 6, 4, 3, 2, 0])`\n",
    "\n",
    "**找到真实标签的排名**：\n",
    "- `rank = (sorted_indices == target).nonzero(as_tuple=True)[0].item() + 1`：找到真实标签 `target` 在排序后的索引列表中的位置，然后加 1（因为排名是从 1 开始计算的）。\n",
    "- `rank = 9`：真实标签 `target` 的排名为 9。\n",
    "\n",
    "**累加倒数排名**：\n",
    "- `mrr += 1.0 / rank`：将当前样本的倒数排名（即 $1 / \\text{rank}$）累加到 `mrr` 变量中。\n",
    "- `mrr = 1.0 / 9`：当前样本的 MRR 为 $1 / 9$。\n",
    "**返回平均 MRR**：\n",
    "- 在遍历完所有样本后，计算累加的 `mrr` 除以样本总数 `outputs.size(0)`，得到所有样本的 **平均倒数排名**  \n",
    "####  calculate_ndcg\n",
    "**对得分排序**：\n",
    "- `sorted_scores, sorted_indices = torch.sort(scores, descending=True)`：对模型预测的所有类别得分进行降序排序，并获取相应的类别索引（排序后的类别）。\n",
    "- `sorted_scores = tensor([0.95, 0.9, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1])`\n",
    "- `sorted_indices = tensor([5, 1, 8, 7, 6, 4, 3, 2, 0])`\n",
    "\n",
    "**相关性提取**：\n",
    "- `relevances = np.array([rank[str(idx.item())] for idx in sorted_indices])`：根据排序后的索引获取对应的相关性标签\n",
    "- `relevances = array([0 0 3 4 5 6 7 8 9])`\n",
    "\n",
    "**计算DCG**：\n",
    "- DCG公式：\n",
    "    $$ \\text{DCG} = \\sum_{i=1}^{n} \\frac{relevance_i}{\\log_2(i+1)} $$\n",
    "- 计算具体数值（假设考虑全部9个元素）：$$ \\frac{0}{\\log_2(2)} + \\frac{0}{\\log_2(3)} + \\frac{3}{\\log_2(4)} + \\ldots $$\n",
    "\n",
    "**计算IDCG**：\n",
    "- 理想情况下相关性得分降序排列：`[9, 8, 7, 6, 5, 4, 3, 0, 0]`\n",
    "- 计算IDCG值\n",
    "\n",
    "**计算nDCG**：\n",
    "- nDCG = DCG / IDCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for embeddings_batch, labels_batch, ranks_batch in train_dataloader:\n",
    "        # print(ranks_batch)\n",
    "        embeddings_batch, labels_batch = embeddings_batch.to(device), labels_batch.to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        outputs = model(embeddings_batch)\n",
    "        loss = criterion(outputs, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_batch.size(0)\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "\n",
    "    # evaluate model\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_mrr = 0.0\n",
    "    val_ndcg = 0.0\n",
    "    val_ndcg_total = 0\n",
    "    with torch.no_grad():\n",
    "        for val_embeddings_batch, val_labels_batch, val_ranks_batch in eval_dataloader:\n",
    "            val_embeddings_batch, val_labels_batch = val_embeddings_batch.to(\n",
    "                device\n",
    "            ), val_labels_batch.to(device)\n",
    "\n",
    "            val_outputs = model(val_embeddings_batch)\n",
    "            val_loss = criterion(val_outputs, val_labels_batch)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "            val_total += val_labels_batch.size(0)\n",
    "            val_ndcg_total += val_outputs.size(0)\n",
    "            val_correct += (val_predicted == val_labels_batch).sum().item()\n",
    "\n",
    "            val_mrr += calculate_mrr(\n",
    "                val_outputs, val_labels_batch\n",
    "            ) * val_labels_batch.size(0)\n",
    "            val_ndcg += calculate_ndcg(\n",
    "                val_outputs, val_ranks_batch, num_classes\n",
    "            ) * val_outputs.size(0)\n",
    "\n",
    "    val_epoch_loss = val_running_loss / len(eval_dataloader)\n",
    "    val_epoch_acc = 100 * val_correct / val_total\n",
    "    val_epoch_mrr = val_mrr / val_total\n",
    "    val_epoch_ndcg = val_ndcg / val_ndcg_total\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%, Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_epoch_acc:.2f}%, Val MRR: {val_epoch_mrr:.4f}, Val nDCG: {val_epoch_ndcg:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nDCG(model, dataloader, device, k=6):\n",
    "    model.eval()\n",
    "    ndcg_total = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, labels_batch, rank_batch in dataloader:\n",
    "            embeddings_batch, labels_batch = embeddings_batch.to(\n",
    "                device\n",
    "            ), labels_batch.to(device)\n",
    "\n",
    "            outputs = model(embeddings_batch)\n",
    "\n",
    "            for i in range(outputs.size(0)):\n",
    "                rank = rank_batch[i]\n",
    "                scores = outputs[i]\n",
    "                sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
    "                relevances = np.array([rank[str(idx.item())] for idx in sorted_indices])\n",
    "                ndcg_total += ndcg(relevances, k)\n",
    "                total += 1\n",
    "\n",
    "    ndcg_avg = ndcg_total / total\n",
    "    print(f\"nDCG@{k}: {ndcg_avg:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_mrr(model, dataloader, device):\n",
    "    model.eval()\n",
    "    mrr = 0.0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, labels_batch, _ in dataloader:\n",
    "            embeddings_batch, labels_batch = embeddings_batch.to(\n",
    "                device\n",
    "            ), labels_batch.to(device)\n",
    "\n",
    "            outputs = model(embeddings_batch)\n",
    "            for i in range(outputs.size(0)):\n",
    "                scores = outputs[i]\n",
    "                target = labels_batch[i].item()\n",
    "                sorted_scores, sorted_indices = torch.sort(scores, descending=True)\n",
    "                print(target, sorted_indices)\n",
    "                rank = (sorted_indices == target).nonzero(as_tuple=True)[0].item() + 1\n",
    "                mrr += 1.0 / rank\n",
    "                total += 1\n",
    "\n",
    "    mrr = mrr / total\n",
    "    print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for embeddings_batch, labels_batch, _ in dataloader:\n",
    "            embeddings_batch, labels_batch = embeddings_batch.to(\n",
    "                device\n",
    "            ), labels_batch.to(device)\n",
    "\n",
    "            outputs = model(embeddings_batch)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels_batch.size(0)\n",
    "            correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试并保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_nDCG(model, test_dataloader, device, num_classes)\n",
    "evaluate_mrr(model, test_dataloader, device)\n",
    "evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "output_model_path = f\"result/classification_model/{dataset}_classification{hard}{data_type}_model_parameters.pth\"\n",
    "torch.save(model.state_dict(), output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"result/{dataset}_contrastive{hard}{data_type}_model\"\n",
    "classification_model_path = f\"result/classification_model/{dataset}_classification{hard}{data_type}_model_parameters.pth\"\n",
    "classification_model = ClassificationModel(input_size, num_classes)\n",
    "classification_model.load_state_dict(torch.load(classification_model_path))\n",
    "\n",
    "embeddings = get_embedding(questions, model_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if isinstance(embeddings, list):\n",
    "    embeddings = torch.tensor(embeddings).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
