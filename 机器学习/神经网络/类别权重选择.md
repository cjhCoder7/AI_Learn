**类别权重（Class Weights）** 

参考文件 [三层神经网络（选择器）](三层神经网络（选择器）.ipynb)

是在训练深度学习模型时，为处理 **类别不平衡** 问题而引入的一种技术。当一个分类任务中某些类别的样本远多于其他类别时，模型往往会偏向预测那些数量更多的类别。为了缓解这种情况，我们可以为不同的类别设置权重，使得模型在训练过程中对样本较少的类别给予更多的关注。

### 为什么需要类别权重？

在分类任务中，样本的类别分布不均衡时，模型容易偏向预测大多数类，而忽视少数类。这种现象尤其严重在：
- **二分类问题**：例如正样本和负样本数量不平衡时，模型可能更倾向于预测负样本。
- **多分类问题**：某个或某些类别的数据远多于其他类别。

如果不处理不平衡问题，模型可能会忽略那些少数类别，导致少数类别的分类精度较低。引入类别权重后，模型在训练时会对少数类样本施加更大的权重，以增强其在训练过程中的影响力。

### 如何计算类别权重？

权重通常根据每个类别的样本数量反比计算，样本数量越少的类别权重越大。一个常用的计算公式是：
$$
w_i = \frac{N}{n_i}
$$


- $w_i$ 是第 $i$ 类的权重。
- $N$ 是总样本数。
- $n_i$ 是第 $i$ 类的样本数。

这种方法可以确保样本数量少的类别在损失函数中被赋予更大的权重，从而使模型更重视这些类别。

### PyTorch 中的类别权重

在 PyTorch 中，可以使用 `CrossEntropyLoss` 的 `weight` 参数来设置类别权重。例如：
```python
criterion = nn.CrossEntropyLoss(weight=class_weights)
```
`class_weights` 是一个张量，长度与类别数量相同，每个元素表示相应类别的权重。

### 代码中的类别权重
```python
from sklearn.utils import compute_class_weight

class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
```

#### 解释：
**`compute_class_weight`**:
   - 使用 `sklearn.utils.class_weight.compute_class_weight` 来根据训练标签的分布计算类别权重。`'balanced'` 选项意味着每个类别的权重与样本的数量成反比（即样本少的类别权重高，样本多的类别权重低）。
**`train_labels`**:
   - `train_labels` 是训练集中样本对应的标签。`compute_class_weight()` 根据这些标签来计算每个类别的样本数量，从而确定类别权重。

### 类别权重的效果

在训练过程中，类别权重被引入到损失函数中，使得模型对不同类别的样本施加不同的重要性。例如，对于少数类别，损失值会被放大，这样在反向传播时会引导模型更多地调整其对这些少数类的预测。

例如，在交叉熵损失函数中，如果某个少数类的样本被误分类，损失会因为权重放大而增加，促使模型进行更显著的更新以改进对该类的预测。

### 总结

**类别权重** 是一种处理类别不平衡问题的有效手段。通过为少数类别样本分配更高的权重，可以让模型在训练过程中更加关注这些类别，避免模型偏向于样本数量多的类别。在实际应用中，设置合理的类别权重可以显著提高模型对少数类的分类性能。